# MLflow with DagsHub: ML Lifecycle Management

## Overview

A hands-on repository focused on learning MLflow with DagsHub, demonstrating how to track machine learning experiments, manage models, and version data/code in a collaborative workflow. It typically covers experiment tracking, parameter and metric logging, model registry usage, and seamless integration of MLflow with DagsHub for reproducible ML projects.

## Key Features

- **Experiment Tracking**: Monitor and compare different ML experiments with their parameters and metrics
- **Parameter & Metric Logging**: Automatically log training parameters and evaluation metrics for reproducibility
- **Model Registry**: Manage model versions, transitions, and deployments through a centralized registry
- **Data & Code Versioning**: Track dataset changes and code versions alongside model experiments
- **Collaborative Workflow**: Seamless integration with DagsHub for team-based ML development
- **Reproducibility**: Ensure all experiments are reproducible with complete tracking of inputs and outputs

## Technologies

- **MLflow**: Open-source platform for managing the complete ML lifecycle
- **DagsHub**: Platform for collaborative data science and ML projects
- **Python**: Primary language for ML experiments

## Getting Started

1. Install required dependencies from `requirements.txt`
2. Run the application with `python app.py`
3. Access MLflow UI to view tracked experiments
4. Connect to DagsHub for collaborative experiment management

## Project Structure

```
├── app.py              # Main application script
├── requirements.txt    # Project dependencies
└── README.md          # This file
```

## Use Cases

This repository is ideal for learning:

- How to structure ML projects with proper experiment tracking
- Best practices for logging and monitoring ML experiments
- Model versioning and registry management
- Collaboration on ML projects with version control
- Reproducible ML workflows
